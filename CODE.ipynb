{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a3f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Training Data:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  expl edit mad usernam hardc metallic fan rever...      0   \n",
      "1  000103f0d9cfb60f  daww match background colo im seem stuck thank...      0   \n",
      "2  000113f07ec002fd  hey man im real try edit war guy const remov r...      0   \n",
      "3  0001b41b1c6bb37e  cant mak real suggest improv wond sect stat la...      0   \n",
      "4  0001d958c54c6e35                     sir hero chant rememb pag that      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "\n",
      "Preprocessed Testing Data:\n",
      "                 id                                       comment_text\n",
      "0  00001cee341fdb12  yo bitch ja rul succes youl ev what hat sad mo...\n",
      "1  0000247867823ef7                                   rfc titl fin imo\n",
      "2  00013b17ad220c46                           sourc zaw ashton lapland\n",
      "3  00017563c3f7919a  look back sourc inform upd correct form guess ...\n",
      "4  00017695ad8997eb                             dont anonym edit artic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# DATA SET\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\train.csv\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test.csv\\test.csv\"\n",
    "test_label_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test_labels.csv\\test_labels.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # REMOVE URLS\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # REMOVE SPECIAL CHARECTER\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # CONVERT TO LOWER CASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    #TOKENIZE THE TEXT \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # REMOVE STOP WORDS\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #LANCASTER STEMING ALTERNATIVE TO PORTER\n",
    "    lancaster = LancasterStemmer()\n",
    "    tokens = [lancaster.stem(word) for word in tokens]\n",
    "    \n",
    "    # LEMMATIZATION\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # JOIN BACK TO FORM THE PREPROCESS TEXT\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# LET'S APPLY PREPROCESS TO  'comment_text'COL\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(preprocess_text)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "print(\"Preprocessed Training Data:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nPreprocessed Testing Data:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4f736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c6e9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9469528434905217\n"
     ]
    }
   ],
   "source": [
    "#PERFORM NAIVE BAYS MODEL \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATA SET\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\train.csv\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test.csv\\test.csv\"\n",
    "test_label_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test_labels.csv\\test_labels.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # REMOVE URLS\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # REMOVE SPECIAL CHARECTER\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # CONVERT TO LOWER CASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    #TOKENIZE THE TEXT \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # REMOVE STOP WORDS\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #LANCASTER STEMING ALTERNATIVE TO PORTER\n",
    "    lancaster = LancasterStemmer()\n",
    "    tokens = [lancaster.stem(word) for word in tokens]\n",
    "    \n",
    "    # LEMMATIZATION\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # JOIN BACK TO FORM THE PREPROCESS TEXT\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# LET'S APPLY PREPROCESS TO  'comment_text'COL\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(preprocess_text)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# SPLIT DATA INTO TRAIN AND VALIDATION SET\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['comment_text'], train_df['toxic'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "\n",
    "#NAIVE BAYES CLASSIFIRE\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#PREDICTION ON THE VALIDATION SET\n",
    "y_pred = model.predict(X_valid_tfidf)\n",
    "\n",
    "# EVALUATE\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace99231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a79eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9540654864483785\n",
      "Precision: 0.8326359832635983\n",
      "Recall: 0.6511780104712042\n",
      "F1 Score: 0.7308116048475947\n",
      "AUC-ROC: 0.8186587581722943\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATA SET\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\train.csv\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test.csv\\test.csv\"\n",
    "test_label_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test_labels.csv\\test_labels.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # REMOVE URLS\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # REMOVE SPECIAL CHARECTER\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # CONVERT TO LOWER CASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    #TOKENIZE THE TEXT \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # REMOVE STOP WORDS\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #LANCASTER STEMING ALTERNATIVE TO PORTER\n",
    "    lancaster = LancasterStemmer()\n",
    "    tokens = [lancaster.stem(word) for word in tokens]\n",
    "    \n",
    "    # LEMMATIZATION\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # JOIN BACK TO FORM THE PREPROCESS TEXT\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# LET'S APPLY PREPROCESS TO  'comment_text'COL\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(preprocess_text)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# SPLIT DATA INTO TRAIN AND VALIDATION SET\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['comment_text'], train_df['toxic'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "\n",
    "# NAIVE BAYS MODEL\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#PREDICTION ON THE VALIDATION SET\n",
    "threshold = 0.3  \n",
    "y_pred = (model.predict_proba(X_valid_tfidf)[:, 1] > threshold).astype(int)\n",
    "\n",
    "# EVALUATEl\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "precision = precision_score(y_valid, y_pred)\n",
    "recall = recall_score(y_valid, y_pred)\n",
    "f1 = f1_score(y_valid, y_pred)\n",
    "roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0dd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc7d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9474228419238603\n",
      "Precision: 0.710445937690898\n",
      "Recall: 0.7611256544502618\n",
      "F1 Score: 0.7349131121642971\n",
      "AUC-ROC: 0.8641381416850914\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATA SET\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\train.csv\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test.csv\\test.csv\"\n",
    "test_label_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test_labels.csv\\test_labels.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # REMOVE URLS\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # REMOVE SPECIAL CHARECTER\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # CONVERT TO LOWER CASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    #TOKENIZE THE TEXT \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # REMOVE STOP WORDS\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #LANCASTER STEMING ALTERNATIVE TO PORTER\n",
    "    lancaster = LancasterStemmer()\n",
    "    tokens = [lancaster.stem(word) for word in tokens]\n",
    "    \n",
    "    # LEMMATIZATION\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # JOIN BACK TO FORM THE PREPROCESS TEXT\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# LET'S APPLY PREPROCESS TO  'comment_text'COL\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(preprocess_text)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# SPLIT DATA INTO TRAIN AND VALIDATION SET\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['comment_text'], train_df['toxic'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# NAIVE BAYS MODEL\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#PREDICTION ON THE VALIDATION SET\n",
    "threshold = 0.2  \n",
    "y_pred = (model.predict_proba(X_valid_tfidf)[:, 1] > threshold).astype(int)\n",
    "\n",
    "\n",
    "# EVALUATE THE MODEL\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "precision = precision_score(y_valid, y_pred)\n",
    "recall = recall_score(y_valid, y_pred)\n",
    "f1 = f1_score(y_valid, y_pred)\n",
    "roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'AUC-ROC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b4e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f73fc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a comment (type 'exit' to stop): Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\"\n",
      "Toxic Comment\n",
      "Enter a comment (type 'exit' to stop): \"Thank you for understanding. I think very highly of you and would not revert without discussion.\"\n",
      "Non-Toxic Comment\n",
      "Enter a comment (type 'exit' to stop): exit\n"
     ]
    }
   ],
   "source": [
    "#BUILD USER INTERECTION\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# DATASET\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\train.csv\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test.csv\\test.csv\"\n",
    "test_label_path = r\"C:\\Users\\nh013\\Desktop\\Identify and classify toxic online comments\\test_labels.csv\\test_labels.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lancaster = LancasterStemmer()\n",
    "    tokens = [lancaster.stem(word) for word in tokens]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# PREPROCESS TO THE 'comment_text' COL\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(preprocess_text)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(preprocess_text)\n",
    "\n",
    "# SPLIT THE DATA INTO TRAIN AND VALIDATION SET\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['comment_text'], train_df['toxic'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF VECTORIZATIONS\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "# TRAN THE MDOEL\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# SAVE THE MODEL\n",
    "model_path = 'toxic_comment_classifier_model.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# FUNCTION TO PREDICT TOXICITY\n",
    "def predict_toxicity(text, threshold=0.2):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    text_vectorized = tfidf_vectorizer.transform([preprocessed_text])\n",
    "    prediction = (model.predict_proba(text_vectorized)[:, 1] > threshold).astype(int)\n",
    "    return prediction\n",
    "\n",
    "# USER INTERECTION\n",
    "while True:\n",
    "    user_input = input(\"Enter a comment (type 'exit' to stop): \")\n",
    "    \n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    prediction = predict_toxicity(user_input)\n",
    "    \n",
    "    if prediction == 1:\n",
    "        print(\"Toxic Comment\")\n",
    "    else:\n",
    "        print(\"Non-Toxic Comment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e20dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
